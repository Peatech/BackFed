---
# *Default configuration to simplify experiment setup
defaults:
  - _self_
  # - atk_config: ??? # Define attack config in child config file
  - override hydra/job_logging: rich  # Use our custom rich logging

# *Aggregator
aggregator: unweighted_fedavg

# *Specify if the attack is enabled
no_attack: False

# *Specify bengin client class and defense client class (if any)
benign_clients: ??? # placeholder that is set during runtime after selecting malicious clients

# *Training mode
training_mode: parallel # Choose from [parallel, sequential]

# *Debug mode (Not works for non-IID datasets (FEMNIST, REDDIT, SENTIMENT140))
debug: False
debug_fraction_data: 0.1 # Only use a portion of the data in debug mode

# *Resource configuration for each client
cuda_visible_devices: 1,2,4,5,6
num_cpus: 1
num_gpus: 0.5

# *Reproducibility
seed: 123
deterministic: False

# *Simulation configuration
num_rounds: 600
num_clients: 100
num_clients_per_round: 10
federated_evaluation: False # Whether to perform federated evaluation (evaluate global model on client's validation set)
federated_val_split: 0.0 # Validation split for each client's dataset (should be > 0 if federated evaluation is True)

# *Dataset distribution configuration
dataset: ??? # CIFAR10, CIFAR100, TinyImageNet, MNIST
datapath: data
partitioner: dirichlet # Choose from [uniform, dirichlet]
alpha: 0.5 # Dirichlet distribution parameter
normalize: True # Normalize the dataset

# *Model configuration
model: ???
pretrain_model_path: Null # A weight path (.pth) | "IMAGENET1K_V2" | Null. This model_path is loaded only if checkpoint is Null.
num_classes: ???

# *Test configuration
test_batch_size: 512
num_workers: 4 # Number of workers for dataloader
pin_memory: True
test_every: 1 # Test every x rounds

# *Default local training configuration (passed to clients during training)
client_config:
  # Training configuration
  dataset: ${dataset}
  local_epochs: 2
  batch_size: 64
  mixed_precision: True   # Use mixed precision and grad scaling during training
  timeout: Null # Timeout for each client training (in seconds)
  optimizer: sgd # Choose from the key in optimizer section below. Will be overridden to store DictConfig in client_optimizer_config
  lr: 0.1 
  weight_decay: 5e-4
  momentum: 0.9
  val_split: ${federated_val_split} # split of the training data to be used for validation
  seed: ${seed}  # Reproducibility
  deterministic: ${deterministic}
  training_mode: ${training_mode}

# *Setting to resume model training (weight and other parameters are loaded).
# *See the full list of parameters in the save_best_model function in server.py
# *Note: If checkpoint is "wandb", the weight will be loaded from wandb

#* Save checkpoints
save_model: True # Save the model to run directory in outputs folder
save_checkpoint: True # Save the model as checkpoints in checkpoints folder (corresponding to Dataset, Model, and Strategy)
save_model_rounds: [200, 400, 600, 800, 1000] # Save the model at the specified rounds

# *Resume from checkpoint
# There are 3 ways to resume:
# 1. checkpoint is a round number, the weight will be loaded from the checkpoints folder corresponding to Dataset, Model, and Strategy
# E.g: checkpoint: 200, the weight will be loaded from the path ./checkpoints/CIFAR10_ResNet18_unweighted_fedavg/model_200.pth
# 2. checkpoint is "wandb", the weight will be loaded from wandb
# 3. checkpoint is a weight path (.pth), the weight will be loaded from the path
# If checkpoint is Null, the weight will not be loaded
checkpoint: Null # A number (round) | weight path (.pth) | "wandb" (load from wandb) | Null

# *Logging configuration
save_logging: csv # Choose from [wandb, csv (save to table), both (wandb & csv), Null (no save logging)]
# Add a name tag to the run name --> Final run name: {name}_{name_tag}
name_tag: ""
# Add a directory tag to csv file and log file --> Final path: {dir_tag}/{dir_path}
dir_tag: ${eval:"'${dataset}_${aggregator}_noattack' if ${no_attack} else '${dataset}_${aggregator}_${atk_config.model_poison_method}(${atk_config.data_poison_method})'"}

wandb:
  entity: ??? # Optional, load from .env file
  project: ??? # Optional, load from .env file
  name: ??? # Automatically set during runtime (See init_wandb() method)
  mode: online
  save_model: False # Whether to save the model to wandb
  save_model_round: -1 # Default: Save model at last round

# *Visualization
plot_data_distribution: False
plot_client_selection: False
progress_bar: True

###############7903916#########################################
# *Aggregator configuration
aggregator_config:
  weighted_fedavg:
    _target_: backfed.servers.WeightedFedAvgServer
    eta: 1

  unweighted_fedavg:
    _target_: backfed.servers.UnweightedFedAvgServer
    eta: 0.5

  fedprox:
    _target_: backfed.servers.FedProxServer
    proximal_mu: 0.01

  foolsgold:
    _target_: backfed.servers.FoolsGoldServer
    confidence: 1

  coordinate_median:
    _target_: backfed.servers.CoordinateMedianServer

  geometric_median:
    _target_: backfed.servers.GeometricMedianServer

  trimmed_mean:
    _target_: backfed.servers.TrimmedMeanServer
    trim_ratio: 0.2

  norm_clipping:
    _target_: backfed.servers.NormClippingServer
    clipping_norm: 3
    eta: 0.5

  robustlr:
    _target_: backfed.servers.RobustLRServer
    robustLR_threshold: 8.0
    eta: 0.5

  krum:
    _target_: backfed.servers.KrumServer
    eta: 0.5

  multi_krum:
    _target_: backfed.servers.MultiKrumServer
    oracle: True # If true, we assume the number of malicious clients is known
    eta: 0.5

  ad_multi_krum:
    _target_: backfed.servers.ADMultiKrumServer # Multi-krum initialized as AnomalyDetectionServer
    eta: 0.5

  weakdp:
    _target_: backfed.servers.WeakDPServer
    strategy: unweighted_fedavg
    std_dev: 0.025
    clipping_norm: 5
    eta: 0.5

  deepsight:
    _target_: backfed.servers.DeepSightServer
    num_seeds: 3
    num_samples: 20000
    deepsight_batch_size: 1000
    deepsight_tau: 0.3333
    eta: 0.5

  flame:
    _target_: backfed.servers.FlameServer
    lamda: 0.001
    eta: 0.5

  rflbat:
    _target_: backfed.servers.RFLBATServer
    eps1: 10.0
    eps2: 4.0
    save_plots: False
    eta: 0.5

  fldetector:
    _target_: backfed.servers.FLDetectorServer
    window_size: 10  # Size of sliding window for detection
    eta: 0.5

  fltrust:
    _target_: backfed.servers.FLTrustServer
    m: 100 # Number of server's root data samples
    eta: 0.5

  flare:
    _target_: backfed.servers.FlareServer
    voting_threshold: 0.5
    m: 10 # Number of auxiliary data samples
    aux_class: 5 # The class used as auxiliary data
    eta: 0.5

  indicator:
    _target_: backfed.servers.IndicatorServer

  localdp:
    _target_: backfed.servers.LocalDPServer
    clipping_norm: 5
    std_dev: 0.01
    eta: 0.5

  fedavgcka:
    _target_: backfed.servers.FedAvgCKAServer
    eta: 0.5
    root_size: 64
    cka_threshold: 0.85
    layer_comparison: penultimate  # Options: penultimate | layer2 | layer3
    use_ood_root_dataset: False
    trim_fraction: 0.5
    
  fedspectre_hybrid:
    _target_: backfed.servers.FedSPECTREHybridServer
    eta: 0.5
    root_size: 64  # Increased to ensure enough samples per class
    alpha: 0.8  # CKA weight
    beta: 0.0   # Stability weight (disabled, not implemented)
    gamma: 0.2  # Spectral weight (CORRECTED: was wrongly mapped to beta)
    rank: 128
    trim_fraction: 0.5
    use_ood_root_dataset: False
    
  fedspectre_simple:
    _target_: backfed.servers.FedSPECTRESimpleServer
    eta: 0.5
    
    # Detection parameters
    cka_weight: 0.7
    spectral_weight: 0.3
    
    # Robust statistics
    trim_fraction: 0.5
    
    # Root dataset
    root_size: 64
    use_ood_root_dataset: false
    
  fedspectre_stateful:
    _target_: backfed.servers.FedSPECTREStatefulServer
    eta: 0.5
    # Base FedSPECTRE-Hybrid parameters
    root_size: 64
    alpha: 0.8  # CKA weight (LEGACY, use w_cka for Phase 1)
    beta: 0.0   # Stability weight (disabled, not implemented)
    gamma: 0.2  # Spectral weight (LEGACY, use w_spectral for Phase 1)
    rank: 128
    trim_fraction: 0.5  # Legacy method (ignored if use_quantile_gating=True)
    use_ood_root_dataset: False
    # Multi-layer parameters (Anti-Neurotoxin)
    use_multi_layer: false    # Enable multi-layer templates
    layer_aggregation: max    # Method to aggregate across layers: max, mean, top_k
    # Trust system parameters (from paper defaults)
    trust_mu: 0.95
    trust_lmbda: 0.85
    trust_alpha: 0.85
    trust_k: 16
    trust_m: 0.52
    trust_theta: 0.65
    trust_gamma: 0.5
    trust_q: 1
    trust_p: 3
    trust_eta: 0.10
    trust_delta: 0.02
    trust_leak_cap: 0.10
    trust_eta_conservative: 0.02
    trust_delta_conservative: 0.01
    trust_leak_cap_conservative: 0.03
    trust_quarantine_exclusion_threshold: 0.95
    trust_quarantine_risk_threshold: 0.9
    trust_quarantine_exclusion_rounds: 10
    trust_quarantine_risk_rounds: 5
    trust_exit_exclusion_threshold: 0.6
    trust_exit_risk_threshold: 0.5
    trust_exit_rounds: 20
    trust_tau_drop: 0.4

  fera:
    _target_: backfed.servers.FeRAServer
    eta: 0.5
    
    # Signal weights
    spectral_weight: 0.6
    delta_weight: 0.4
    
    # Detection parameters
    top_k_percent: 0.5  # Flag bottom 50% (consistency-based detection)
    
    # Outlier removal (catches norm-inflation attacks like Anticipate)
    remove_outliers: true  # Two-sided adaptive filtering on raw norms
    outlier_threshold: 2.0  # Modified Z-score threshold on raw norms (3.0Ïƒ)
    
    # Multi-layer options
    use_multi_layer: false
    layers: ['penultimate']  # Options: penultimate, layer2, layer3, layer4, all
    combine_layers_method: 'mean'  # Options: mean, max, vote
    
    # Root dataset
    root_size: 64
    use_ood_root_dataset: false

########################################################
# *Optimizer configuration
client_optimizer_config:
  sgd: # Default for CIFAR10, CIFAR100, EMNIST, TinyImageNet
    _target_: torch.optim.SGD
    lr: ${client_config.lr}
    weight_decay: ${client_config.weight_decay}
    momentum: ${client_config.momentum}
  adam:
    _target_: torch.optim.Adam
    lr: ${client_config.lr}
    weight_decay: ${client_config.weight_decay}
  adamw:
    _target_: torch.optim.AdamW
    lr: ${client_config.lr}
    weight_decay: ${client_config.weight_decay}

########################################################
# *Hydra configuration
hydra:
  job:
    chdir: False  # Prevent Hydra from changing working directory
  run:
    dir: ./outputs/${dir_tag}/${now:%Y-%m-%d-%H-%M-%S}
  sweep:
    dir: ./multirun/${dir_tag}/${now:%Y-%m-%d-%H-%M-%S}
    subdir: ${hydra.job.num}
    # dir: ./multirun/${dir_tag}/${now:%Y-%m-%d-%H-%M-%S}/${hydra.job.override_dirname}
    # subdir: ${hydra.job.num}
